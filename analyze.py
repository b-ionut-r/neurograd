#!/usr/bin/env python3
"""
NeuroGrad Profiling Log Analyzer

This script analyzes profiling_log.txt generated by MemoryProfiler to identify
performance bottlenecks in forward and backward passes.

Usage:
    python analyze.py [profiling_log.txt]
"""

import re
import sys
import argparse
from collections import defaultdict
from pathlib import Path


def parse_profiling_log(log_file):
    """Parse the profiling log file and extract timing data."""
    timing_data = []
    
    try:
        with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
    except FileNotFoundError:
        print(f"Error: Could not find {log_file}")
        sys.exit(1)
    except Exception as e:
        print(f"Error reading {log_file}: {e}")
        sys.exit(1)
    
    # Pattern to match timing entries from NeuroGrad profiling format
    # Format: [PROFILE] OperationName_fwd/bwd: ... | time=X.XX ms | ...
    pattern = r'\[PROFILE\]\s+(\w+(?:_(?:fwd|bwd))?):.*?\|\s*time=([\d.]+)\s*(ms|μs)'
    matches = re.findall(pattern, content)
    
    print(f"Found {len(matches)} timing entries in {log_file}")
    
    for operation, time_str, unit in matches:
        try:
            time_val = float(time_str)
            # Convert microseconds to milliseconds
            if unit == 'μs':
                time_ms = time_val / 1000.0
            else:  # unit == 'ms'
                time_ms = time_val
            timing_data.append((operation, time_ms))
        except ValueError:
            continue
    
    return timing_data


def analyze_by_pass_type(timing_data):
    """Analyze timing data separated by forward and backward passes."""
    fwd_times = defaultdict(list)
    bwd_times = defaultdict(list)
    
    for operation, time_ms in timing_data:
        # Check if this is a backward pass operation
        if operation.endswith('_bwd'):
            # Remove _bwd suffix for cleaner operation name
            op_name = operation[:-4]
            bwd_times[op_name].append(time_ms)
        elif operation.endswith('_fwd'):
            # Remove _fwd suffix for cleaner operation name
            op_name = operation[:-4]
            fwd_times[op_name].append(time_ms)
        else:
            # Fallback to keyword detection for operations without suffixes
            backward_keywords = {'backward', 'grad', 'Backward'}
            is_backward = any(keyword in operation for keyword in backward_keywords)
            
            if is_backward:
                bwd_times[operation].append(time_ms)
            else:
                fwd_times[operation].append(time_ms)
    
    return fwd_times, bwd_times


def calculate_totals(operation_times):
    """Calculate total time for each operation type."""
    totals = {}
    for operation, times in operation_times.items():
        totals[operation] = sum(times)
    return totals


def print_analysis_report(fwd_totals, bwd_totals, total_totals):
    """Print comprehensive timing analysis report."""
    
    def format_time(ms):
        """Format time with appropriate units."""
        if ms >= 1000:
            return f"{ms/1000:.2f}s"
        else:
            return f"{ms:.2f}ms"
    
    def print_top_operations(totals, title, total_time):
        """Print top operations for a given category."""
        print(f"\n{title}")
        print("=" * len(title))
        
        sorted_ops = sorted(totals.items(), key=lambda x: x[1], reverse=True)
        
        for i, (operation, total_ms) in enumerate(sorted_ops[:10], 1):
            percentage = (total_ms / total_time) * 100
            print(f"{i:2d}. {operation:<20} {format_time(total_ms):>10} ({percentage:5.1f}%)")
        
        print(f"\nTotal {title.split()[0]} Time: {format_time(total_time)}")
        return sorted_ops
    
    # Calculate total times
    total_fwd_time = sum(fwd_totals.values())
    total_bwd_time = sum(bwd_totals.values())
    total_overall_time = sum(total_totals.values())
    
    print("\n" + "="*80)
    print("NEUROGRAD PROFILING ANALYSIS REPORT")
    print("="*80)
    
    # Forward pass analysis
    fwd_sorted = print_top_operations(fwd_totals, "FORWARD PASS BOTTLENECKS", total_fwd_time)
    
    # Backward pass analysis  
    bwd_sorted = print_top_operations(bwd_totals, "BACKWARD PASS BOTTLENECKS", total_bwd_time)
    
    # Overall analysis
    overall_sorted = print_top_operations(total_totals, "OVERALL BOTTLENECKS", total_overall_time)
    
    # Summary statistics
    print(f"\n{'SUMMARY STATISTICS':^80}")
    print("="*80)
    print(f"Total Forward Pass Time:  {format_time(total_fwd_time):>10} ({(total_fwd_time/total_overall_time)*100:5.1f}%)")
    print(f"Total Backward Pass Time: {format_time(total_bwd_time):>10} ({(total_bwd_time/total_overall_time)*100:5.1f}%)")
    print(f"Total Overall Time:       {format_time(total_overall_time):>10} (100.0%)")
    
    # Performance insights
    print(f"\n{'PERFORMANCE INSIGHTS':^80}")
    print("="*80)
    
    if overall_sorted:
        top_bottleneck = overall_sorted[0]
        print(f"🔥 Primary Bottleneck: {top_bottleneck[0]} ({(top_bottleneck[1]/total_overall_time)*100:.1f}% of total time)")
    
    if fwd_sorted:
        fwd_top = fwd_sorted[0]
        print(f"⏩ Forward Pass Leader: {fwd_top[0]} ({(fwd_top[1]/total_fwd_time)*100:.1f}% of forward time)")
    
    if bwd_sorted:
        bwd_top = bwd_sorted[0]
        print(f"⏪ Backward Pass Leader: {bwd_top[0]} ({(bwd_top[1]/total_bwd_time)*100:.1f}% of backward time)")
    
    # Optimization recommendations
    print(f"\n{'OPTIMIZATION RECOMMENDATIONS':^80}")
    print("="*80)
    
    for i, (operation, total_ms) in enumerate(overall_sorted[:3], 1):
        percentage = (total_ms / total_overall_time) * 100
        print(f"{i}. Optimize {operation}")
        print(f"   Current impact: {format_time(total_ms)} ({percentage:.1f}% of total time)")
        
        # Operation-specific recommendations
        if 'EinSum' in operation:
            print("   → Consider using cuDNN optimized convolutions")
            print("   → Implement operation fusion for einsum chains")
        elif 'BatchNormalizer' in operation:
            print("   → Use fused batch normalization kernels")
            print("   → Consider layer normalization alternatives")
        elif 'Var' in operation:
            print("   → Cache variance calculations where possible")
            print("   → Use numerically stable variance algorithms")
        elif 'Conv' in operation:
            print("   → Enable cuDNN auto-tuning")
            print("   → Consider depthwise separable convolutions")
        else:
            print("   → Profile operation internals for optimization opportunities")
        print()


def categorize_operations(timing_data):
    """Categorize operations by type for additional analysis."""
    categories = {
        'Compute': ['EinSum', 'MatMul', 'Conv', 'Linear'],
        'Normalization': ['BatchNormalizer', 'LayerNorm'],
        'Statistics': ['Var', 'Mean', 'Std'],
        'Memory': ['SlidingWindowView', 'Reshape', 'Transpose'],
        'Activation': ['ReLU', 'Sigmoid', 'Tanh'],
        'Loss': ['CrossEntropy', 'MSE']
    }
    
    category_times = defaultdict(float)
    uncategorized_times = defaultdict(float)
    
    for operation, time_ms in timing_data:
        categorized = False
        for category, keywords in categories.items():
            if any(keyword in operation for keyword in keywords):
                category_times[category] += time_ms
                categorized = True
                break
        
        if not categorized:
            uncategorized_times[operation] += time_ms
    
    return category_times, uncategorized_times


def main():
    """Main analysis function."""
    # Get log file path from command line or use default
    log_file = sys.argv[1] if len(sys.argv) > 1 else 'profiling_log.txt'
    
    if not Path(log_file).exists():
        print(f"Error: {log_file} not found")
        sys.exit(1)
    
    print(f"Analyzing {log_file}...")
    
    # Parse timing data
    timing_data = parse_profiling_log(log_file)
    
    if not timing_data:
        print("No timing data found in log file")
        sys.exit(1)
    
    # Analyze by pass type
    fwd_times, bwd_times = analyze_by_pass_type(timing_data)
    
    # Calculate totals
    fwd_totals = calculate_totals(fwd_times)
    bwd_totals = calculate_totals(bwd_times)
    
    # Combine for overall analysis
    all_times = defaultdict(list)
    for operation, times in fwd_times.items():
        all_times[operation].extend(times)
    for operation, times in bwd_times.items():
        all_times[operation].extend(times)
    
    total_totals = calculate_totals(all_times)
    
    # Print comprehensive report
    print_analysis_report(fwd_totals, bwd_totals, total_totals)
    
    # Additional category analysis
    category_times, uncategorized_times = categorize_operations(timing_data)
    
    if category_times:
        print(f"\n{'OPERATION CATEGORY BREAKDOWN':^80}")
        print("="*80)
        
        total_categorized = sum(category_times.values())
        sorted_categories = sorted(category_times.items(), key=lambda x: x[1], reverse=True)
        
        for category, total_ms in sorted_categories:
            percentage = (total_ms / total_categorized) * 100
            if total_ms >= 1000:
                time_str = f"{total_ms/1000:.2f}s"
            else:
                time_str = f"{total_ms:.2f}ms"
            print(f"{category:<15} {time_str:>10} ({percentage:5.1f}%)")
    
    print(f"\n{'Analysis complete!':^80}")
    print("="*80)


if __name__ == "__main__":
    main()