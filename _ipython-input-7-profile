


# Improved training loop with learning rate scheduling
print("Starting improved training...")
print("=" * 60)

# Initialize tracking
train_losses = []
train_accuracies = []
test_losses = []
test_accuracies = []
epoch_times = []
learning_rates = []

start_time = time.time()
initial_lr = learning_rate

for epoch in range(epochs):
    epoch_start = time.time()
    
    # Learning rate scheduling - reduce LR when training plateaus
    if epoch == 40:  # First reduction at epoch 40
        learning_rate = initial_lr * 0.5
        optimizer.lr = learning_rate
        print(f"  Reducing learning rate to {learning_rate}")
    elif epoch == 70:  # Second reduction at epoch 70
        learning_rate = initial_lr * 0.1
        optimizer.lr = learning_rate
        print(f"  Reducing learning rate to {learning_rate}")
    
    learning_rates.append(learning_rate)
    
    # Training phase
    model.train()
    epoch_train_losses = []
    epoch_train_accs = []
    
    for batch_idx, (batch_X, batch_y) in enumerate(train_loader):
        # Forward pass
        optimizer.zero_grad()
        predictions = model(batch_X)
        loss = loss_fn(batch_y, predictions)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Track metrics
        batch_loss = loss.data.item()
        batch_acc = calculate_accuracy(predictions, batch_y)
        
        epoch_train_losses.append(batch_loss)
        epoch_train_accs.append(batch_acc)
    
    # Calculate epoch averages
    avg_train_loss = np.mean(epoch_train_losses)
    avg_train_acc = np.mean(epoch_train_accs)
    
    # Evaluation phase
    test_loss, test_acc = evaluate_model(model, X_test_tensor, y_test_tensor)
    
    # Record metrics
    train_losses.append(avg_train_loss)
    train_accuracies.append(avg_train_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)
    
    epoch_time = time.time() - epoch_start
    epoch_times.append(epoch_time)
    
    # Print progress more frequently for longer training
    if epoch % 20 == 0 or epoch == epochs - 1 or epoch in [39, 69]:  # Show LR reduction epochs
        print(f"Epoch {epoch:3d}/{epochs}: "
              f"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}, "
              f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, "
              f"LR: {learning_rate:.6f}, Time: {epoch_time:.2f}s")

total_time = time.time() - start_time
print("=" * 60)
print(f"Improved training completed in {total_time:.2f} seconds")
print(f"Average time per epoch: {np.mean(epoch_times):.2f}s")
print(f"Final train accuracy: {train_accuracies[-1]:.4f}")
print(f"Final test accuracy: {test_accuracies[-1]:.4f}")
print(f"Best test accuracy: {max(test_accuracies):.4f} (epoch {np.argmax(test_accuracies)})")
print(f"Improvement from baseline (66.1%): {max(test_accuracies) - 0.661:.3f}")
